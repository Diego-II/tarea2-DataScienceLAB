{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iHVugQSjzPHo"
   },
   "source": [
    "### P3. Interpretabilidad.\n",
    "****************************\n",
    "El objetivo de esta pregunta es que implemente un modelo auxiliar de interpretabilidad local sobre las predicciones que genera una red neuronal. Esto consiste en generar perturbaciones sobre datos de entrada, con el fin de comprender la importancia de las variables en los procesos de predicción. Para ello deberá implementar el método **LIME** (**L**ocal **I**interpretable **M**odel-Agnostic **E**xplanations)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Desarrollo teórico.\n",
    "**********************\n",
    "\n",
    "El procediminto **LIME** consiste en una metodología diseñada para otorgar _interpretabilidad_ a modelos de parendizaje que suelen ser denotados como \"caja negra\". Por interpretabilidad se entiende, la capacidad de establecer relaciones claras entre las variables de un fenómeno y la respuesta que producen. **LIME** es \"agóstico en el modelo\", esto se refiere a que pueda ser utilizado para cualquier tipo de modelo de predicción.\n",
    "\n",
    "La idea central de **LIME** consiste en aproximar localmente el comportamiento de un predictor, utilizando un modelo que sea interpretable como por ejemplo regresión lineal o árboles de decisión. En términos concretos, dada una instancia a predecir $x \\in \\mathbb{R}^d$ (dato de entrada), se utilizará un vector $x' \\in \\{0,1\\}^{d'}$ como representación interpretable. Se define así, una **explicación** como un modelo $g \\in G$, donde $G$ coresponde a una familia de modelos potencialmente interpretables, el dominio de cada $g\\in G$ será $\\{0,1\\}^{d'}$. Para asegurar que la aproximación buscada sea interpretable porun humano, se utiliza una medida de complejidad $\\Omega(g)$ sobre cada $g\\in G$, considerando el grado de complejidad en contraposición a la interpretablidad de un modelo.\n",
    "\n",
    "Sea un modelo predictor $f: \\mathbb{R}^d \\rightarrow \\mathbb{R}$, sea además $x \\in \\mathbb{R}$ en el conjunto de datos de entrada, y $x' \\in \\{0,1\\}^{d'}$ su representación interpretable. Para $s$ se define $\\pi_x(z'): \\{0,1\\}^{d'} \\rightarrow \\mathbb{R}$ como una medda de similitud entre $x'$ y $z'\\in \\{0,1\\}^{d'}$. Finalmente, se define $\\mathcal{L}(f,g,\\pi_x)$ en una vecindad inducida localmente por $\\pi_x$. Para aseguar interpretabilidad y fidelidad local (asociada a $x$), l expresión prodcida por **LIME** se obtiene resolviendo la siguiente expresión:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\xi(x) = \\arg\\min_{g\\in G}\\,\\mathcal{L}(f,g,\\pi_x) + \\Omega(g)\n",
    "    \\tag{1}\n",
    " \\end{equation}\n",
    " $$\n",
    " \n",
    " En esta pregunta, se utilizará regresión logística como familia de explicaciones $G$, es decir para cada $g\\in G$ este será de la forma $g(x') = \\sigma(w_g \\cdot x')$. COmo función de fidelidad $\\mathcal{L}$ se usa la verosimilitud asociada a la regresión logística, ponderada localmente por $\\pi_x$, es decir:\n",
    " \n",
    "$$\n",
    "\\begin{equation}\n",
    "    \\mathcal{L}(f,g,\\pi_x) = \\sum_{z,z'} \\pi_x(z)\\left(f(z)log(g(z'))\\right) + \\left(1-f(z)\\right)log(1-g(z'))\n",
    "    \\tag{2}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "En este caso $\\pi_x$ será un kernel exponencial definido por una medida de similitud, se utilizará la distancia coseno:\n",
    "\n",
    "$$\n",
    "\\pi_x(z') = exp\\left(-d(x',z')^2 / \\sigma^2\\right)\n",
    "\\tag{3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "d(x',z') = 1 - \\frac{x'\\cdot z'}{||x'||\\,||z'||}\n",
    "\\tag{4}\n",
    "$$\n",
    "\n",
    "Implemente los pasos iniciales para trabajos con **LIME**, para esto:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hyfQrJhrz19N"
   },
   "source": [
    "   1. Instancie un objeto ```torchvision.transforms.Compose```, este opera sobre imágenes ```PIL``` y le aplique las siguientes transformaciones:\n",
    "   * Escalonamiento de la imágen a un tamaño de 299&times;299 píxeles.\n",
    "   * Opere por medio de ```CenterCrop(299)```.\n",
    "   * Transforme las imágenes en un objeto ```Tensor```.\n",
    "   * Normalice con las  medias ```[0.485, 0.456, 0.406]``` y, desviaciones estándar ```[0.229, 0.224, 0.225]```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4H8ZwV6izWuP"
   },
   "outputs": [],
   "source": [
    "# librerías usadas\n",
    "import torch\n",
    "from torchvision import models\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# transformaciones compose:\n",
    "'''\n",
    "transforms.Resize([299,299]) para cambiar el tamaño de la imágen a una dimensión de 299x299.\n",
    "\n",
    "transforms.CenterCrop(229) recorta la imágen desde su centro hasta las dimensiones 299x299 (nxn caso general).\n",
    "\n",
    "transform.ToTensor() transforma la imágen en un Tensor de Pytorch\n",
    "\n",
    "transform.Normalize normaliza un tensor, en este sentido es necesario que ToTensor esté antes de esta transformación.\n",
    "La normalización para cada input en la dimensión correspondiente es (input - mean)/std\n",
    "'''\n",
    "\n",
    "transformers=transforms.Compose(\n",
    "    [transforms.Resize([299,299]),\n",
    "     transforms.CenterCrop(229),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])         \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZIFqB6Kez6OH"
   },
   "source": [
    "  2. Cargue la red ```inception_v3``` entrenada sobre _imageNet_ (en Pytorch). Utilice esta red para hacer predicción sobre una imagen control a cual se aplican las transformaciones antes definidas. Obtenga una clase más probable asignada por la red.\n",
    "  \n",
    "     Para esto último puede ser útil la función ```decode_predictions``` del módulo ```keras.applications.imagenet_utils``` sobre las predicciones de la red cargada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wXKD8_5Lz5kp"
   },
   "outputs": [],
   "source": [
    "# Se carga una imágen cualquiera con la librería urllib desde su url respectiva.\n",
    "%%capture\n",
    "import urllib\n",
    "url, filename = (\"https://github.com/pytorch/hub/raw/master/dog.jpg\", \"dog.jpg\")\n",
    "try: urllib.URLopener().retrieve(url, filename)\n",
    "except: urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "### cargo el modelo inception_v3 con la librería torch\n",
    "'''\n",
    "Librería torch.hub es un repositorio de modelos pre-entrenados que facilita la reproducción de estos\n",
    "entrega convenientes API's para explorar y cargar modelos.\n",
    "'''\n",
    "\n",
    "# cargo la red\n",
    "inception_v3_net = torch.hub.load('pytorch/vision:v0.6.0', 'inception_v3', pretrained=True)\n",
    "\n",
    "# cargo el modelo entrenado con el método .eval()\n",
    "# puedo evitar los mensajes que imprime el método con la celda mágica %%capture al inicio.\n",
    "inception_v3_net.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "819KCCLGQF0L"
   },
   "outputs": [],
   "source": [
    "#transformo los datos\n",
    "input_image = Image.open(filename)\n",
    "plt.imshow(np.asarray(input_image))\n",
    "\n",
    "input_tensor = transformers(input_image)\n",
    "input_batch = input_tensor.unsqueeze(0)\n",
    "\n",
    "#predicción del modelo\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    input_batch = input_batch.to('cuda')\n",
    "    inception_v3_net.to('cuda')\n",
    "    \n",
    "with torch.no_grad():\n",
    "  out = inception_v3_net(input_batch).cpu() #salida(predicción)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HweTNHiR5Frc"
   },
   "outputs": [],
   "source": [
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "labs = decode_predictions(out.numpy(),top=1)\n",
    "\n",
    "print('\\n')\n",
    "print('Nombre de la clase predicha: ', labs[0][0][0],'\\n')\n",
    "print('Descripción de la clase: ', labs[0][0][1],'\\n')\n",
    "print('Valor del puntaje de la red: ',labs[0][0][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la descripción de la clase aparece la palabra _Semoyed_ que corresponde al nombre (en inglés) de la raza del perro que aparece en la imágen elegida. Esto muestra la potencia de la red que, además de clasificar el objeto como un perro, obtiene una subclase como lo es la raza."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fev38v99Rvdr"
   },
   "source": [
    "  3. Segmente la imagen de control utilizando la función ```slic``` del módulo ```skimage.segmentation```, para los parámetros ```start_label=0``` (no válido en todas las versiones), ```n_segments=80```. El resultado de la segmentación es un arreglo de dimensión 299&times;299 que asigna una categoría para cada píxel de la imágen procesada. Todos los pixeles en la imágen que comparten etiqueta conforman un súper-píxel dentro de la imágen. Utilice la función ```mark_boundaries``` del módulo ```skimage.segmentation``` en conjunto con ```imshow``` del módulo ```skimage.io``` para visualizar los bordes inducidos por el conjunto de super-pixeles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fsJDfLBu1DnH"
   },
   "outputs": [],
   "source": [
    "import skimage.segmentation as sks\n",
    "from skimage.io import imshow\n",
    "\n",
    "#función slic recibe un objeto del tipo ndarray, por lo tanto pasamos la imagen a un array de Numpy\n",
    "#si la imagen es de dimensiones (N,M,3) se asume que es una imagen 2D RGB\n",
    "imagen_np = np.asarray(input_image)\n",
    "imagen_seg = sks.slic(imagen_np, n_segments=80)\n",
    "\n",
    "# a pesar de imponer n_segments = 80, puede ser que slic asocie una partición de menor tamaño.\n",
    "\n",
    "print('Imágen segmentada:\\n')\n",
    "imshow(imagen_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j216xhC-6IZ-"
   },
   "outputs": [],
   "source": [
    "print('Imagen original:\\n')\n",
    "plt.imshow(imagen_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gjsizs1LVV7b"
   },
   "outputs": [],
   "source": [
    "imagen_bnd = sks.mark_boundaries(imagen_np,imagen_seg, color = (0,0,0), outline_color=(0,1,0))\n",
    "print('Imágen con los super-pixeles marcados en sus bordes:\\n')\n",
    "ims(imagen_bnd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hW7CJQCeZbXM"
   },
   "source": [
    "Al representar una imágen _x_ por medio de la presencia y ausencia de súper-pixeles se logra una representación interpretable _x'_ según un vector de entradas binarias.\n",
    "\n",
    "Genere perturbaciones en la imágen de control, para esto siga los siguientes pasos:\n",
    "\n",
    "  4. Defina un número de _perturbaciones_ a realizar (al menos 1000). Cada perturbación consiste en un arreglo binario, donde cada componente es asociada a un súper-pixel. Estos arreglos serán las representaciones interpretables de la imagen de control (_x'_ asociado a _x_). Considere cada entrada del arreglo de perturbaciones como una variable **Bernoulli** con _p=0.5_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i-dcuXsgWjDy"
   },
   "outputs": [],
   "source": [
    "#probaremos con 1000 perturbaciones\n",
    "n_per = 1000\n",
    "ind = np.array([(i in imagen_seg) for i in range(80)])\n",
    "n_clases = len(ind[ind == True])\n",
    "\n",
    "M = np.zeros([n_clases,n_per])\n",
    "\n",
    "#binomial(1,p)=bernoulli(p), asigno en una matriz de n_clases x n_perturbaciones donde cada fila corresponde al valor de la clase en esa perturbación.\n",
    "np.random.seed(10)\n",
    "for i in range(n_per):\n",
    "  M[:,i] = np.random.binomial(1,0.5,n_clases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SjIDDtLlk9gd"
   },
   "source": [
    "  5. Genere tantas versiones perturbadas de la imagen de control como perturbaciones haya conseguido. Obtener una imagen perturbada consiste en asignar el valor 0 en cada canal de color en aquellos píxeles cuyos super-pixeles asociados tengan su componente nula en el vector de perturbaciones. Obtenga una visualización de una imágen perturbada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-z9uB0FidkeN"
   },
   "outputs": [],
   "source": [
    "#generando imágenes perturbadas.\n",
    "\n",
    "n_x, n_y, n_colores= np.shape(imagen_np) #coordenadas de la imagen normal.\n",
    "\n",
    "imag_perturb = [] #guardar imágenes perturbadas. guardar en formato Image\n",
    "\n",
    "\n",
    "for p in range(n_per):\n",
    "  #para cada perturbación hago una copia de la imagen\n",
    "  image_copy = imagen_np.copy()\n",
    "  for i in range(n_x):\n",
    "    for j in range(n_y):\n",
    "      pos = imagen_seg[i,j]\n",
    "      if M[pos,p] == 0:\n",
    "        image_copy[i,j,:] = [0,0,0]\n",
    "  \n",
    "  imag_perturb.append(image_copy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N2vT2gWZgxZl"
   },
   "outputs": [],
   "source": [
    "# Mostrar una imágen cualquiera\n",
    "plt.imshow(imag_perturb[425])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h7CLIJq-uavt"
   },
   "source": [
    "  6. Haga predicciones sobre las imágenes perturbadas utilizando la red ```inception_v3```. Asocie el valor 1 como etiqueta a las imágenes perturbadas  que sean clasificadas a la misma categoría de la imágen de control y 0 en caso contrario, el arreglo binario correspondiente se denotará _y_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JHyaMPjJsFOw"
   },
   "outputs": [],
   "source": [
    "#función que evalúa una imágen y la compara con una etiqueta según la predicción\n",
    "def Evaluacion_red(imag_array, nombre_cat, print_ = False):\n",
    "  input_image = Image.fromarray(imag_array)\n",
    "  input_tensor = transformers(input_image)\n",
    "  input_batch = input_tensor.unsqueeze(0)\n",
    "\n",
    "  if torch.cuda.is_available():\n",
    "    input_batch = input_batch.to('cuda')\n",
    "    inception_v3_net.to('cuda')\n",
    "    \n",
    "  with torch.no_grad():\n",
    "    out = inception_v3_net(input_batch).cpu()\n",
    "  \n",
    "  labs = decode_predictions(out.numpy(),top=1)\n",
    "  \n",
    "  if print_:\n",
    "    print(labs[0][0][0])\n",
    "  \n",
    "  if labs[0][0][0] == nombre_cat:\n",
    "    return 1\n",
    "  else:\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7FJ7mn1Kv8mZ"
   },
   "outputs": [],
   "source": [
    "# nombre de la categoría correcta.\n",
    "label = labs[0][0][0]\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "orM37hh6wAih"
   },
   "outputs": [],
   "source": [
    "#Prueba de la función\n",
    "Evaluacion_red(imag_perturb[20], label, print_ = True) #no acertó"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CzHX9rwQwN-8"
   },
   "outputs": [],
   "source": [
    "#Generación del vector Y\n",
    "y =[]\n",
    "for i in range(n_per):\n",
    "  y.append(Evaluacion_red(imag_perturb[i],label))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HGjUZGFvwvL-"
   },
   "source": [
    "  7. Calcule &pi;<sub>_x_</sub> según la expresión (3). Para ello, obtenga la distancia del coseno entre las perturbaciones asociadas a cada imágen perturbada y el vector de perturbación de la imágen de control _x_ según lo indica (4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A9qXTARF7tQZ"
   },
   "source": [
    "Tomamos\n",
    "$$\n",
    "x' = (1,\\cdots,1)\n",
    "\\tag{5}\n",
    "$$\n",
    "\n",
    "como el vector que identifica a la imagen original $x$, puesto que $x$ es la imagen completa y el vector $x'$ definido en (5) concerva la imagen completa al no deshechar ninguna de las particiones. Entonces, si $N$ es la cantidad de clases encontradas en la segmentación, entonces:\n",
    "\n",
    "$$\n",
    "x' \\in \\{0,1\\}^N,\\hspace{0.4cm}\\text{y }\\hspace{0.4cm}||x'|| = \\sqrt{N}\n",
    "$$\n",
    "\n",
    "(asumimos la norma euclideana en $\\mathbb{R}^{N}$).\n",
    "\n",
    "\n",
    "Ahora, supongamos que $z' \\neq x'$, por lo tanto existe un valor $M<N$, donde $M$ es la cantidad de valores 1 en el vector $z'$, entonces:\n",
    "\n",
    "$$\n",
    "\\frac{x\\cdot z}{||x||\\,||z||} = \\frac{M}{\\sqrt{N}\\sqrt{M}} = \\sqrt{\\frac{M}{N}}\n",
    "$$\n",
    "\n",
    "Con esta nueva definición, podríamos decir que si el vecto $z'$ es el vector nulo, entonces $M=0$ y\n",
    "\n",
    "$$\n",
    "\\frac{x\\cdot z}{||x||\\,||z||} \\approx 0,\n",
    "$$\n",
    "\n",
    "donde cláramente es cierto para valores contínuos de $M\\rightarrow 0$ (lo que no es el caso), así extendemos la definición de $d(x',z')$ para el valor $0$ de la siguiente forma:\n",
    "\n",
    "$$\n",
    "d(x',z') = \\left\\{\\begin{array}[cc]\n",
    "                        11 - \\frac{x'\\cdot z'}{||x'||\\,||z'||}&,\\hspace{0.2cm}\\text{si }z'\\neq \\vec{0}\\\\\n",
    "                        &\\\\\n",
    "                        1 &,\\hspace{0.2cm}\\text{si }z' = \\vec{0}\n",
    "                   \\end{array}\\right.\n",
    "$$\n",
    "\n",
    "De esta manera posicionamos al vector 0 como el vector \"más lejanos\" a la representación interpretable $x'$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-PuPavEjwm7c"
   },
   "outputs": [],
   "source": [
    "# Definimos la distancia.\n",
    "def dist_x(z):\n",
    "  n = len(z)\n",
    "  a = np.sum(z)\n",
    "  norm_x = np.sqrt(n)\n",
    "  norm_z = np.sqrt(np.sum(z**2))\n",
    "\n",
    "  d = 1 - (a/(norm_x*norm_z))\n",
    "  if norm_z == 0:\n",
    "    return 1\n",
    "  else:\n",
    "    return d\n",
    "\n",
    "# Definimos la distancia coseno:\n",
    "\n",
    "def pi_x(z,sigma):\n",
    "  d = dist_x(z)\n",
    "\n",
    "  return np.exp((-d**2) / (sigma**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XuKEK9Tcwp_9"
   },
   "outputs": [],
   "source": [
    "# Primera perturbación:\n",
    "per_0 = M[:,0]\n",
    "np.shape(per_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FJ2MQvsS-cXf"
   },
   "outputs": [],
   "source": [
    "# Prueba de las funciones anteriores para sigma=1.\n",
    "pi_x(per_0, sigma=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HBg3X6Xs_CYi"
   },
   "source": [
    "Una vez obtenido el conjunto de reprsentaciones para la imágen de control $x$ y el vector de pesos asociados &pi;<sub>$x$</sub>, se pasa a minimizar la función de fidelidad, para esto:\n",
    "\n",
    "  8. Genere un conjunto de entrenamiento $D_p$. Este consta de vectores de perturbación como observaciones. La variable de respuesta será el arreglo $y$ generado anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "58iGNS7z-n8t"
   },
   "outputs": [],
   "source": [
    "D_p = [M[:,i] for i in range(n_per)]\n",
    "\n",
    "# y definido anteriormente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cy1SlME4AxxU"
   },
   "source": [
    "  9. Utilice la clase ```LogisticRegression``` del módulo ```sklearn.linear_model``` para entrenar un clasificador sobre el conjunto de entrenamiento $D_p$. Haga uso de $\\pi_x$ al momento de usar el método ```.fit()```. ¿Es posible agregar una medida de complejidad $\\Omega(g)$ con este esquema?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vodu3W2IAw6J"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "sigma = 0.25\n",
    "weight = np.array([pi_x(D_p[i],sigma) for i in range(n_per)])\n",
    "\n",
    "reg = LogisticRegression().fit(D_p,y, sample_weight=weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7n-y8KuDE92d"
   },
   "outputs": [],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "itYrAE8wOmO1"
   },
   "source": [
    "   10. Utilice los coeficientes del clasificador anterior para inferir los súper-índices de mayor importancia en la clasificación de la imágen de control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YogKmq5rO7Cj"
   },
   "source": [
    "Dado que la regresión logística vincula a la variable $X$ con la respuesta $Y$ de una forma no lineal, no se analizarán a los coeficientes por el tamaño de su magnitud (módulo) como en la regresión lineal. En cambio, la importancia de la variable estará dada por un factor exponencial del coeficiente. Es decir, si $\\beta_i$ es el coeficiente asociado al súper-píxel $i$, entonces la importancia de tal súperpixel se puede medir mediante:\n",
    "\n",
    "$$\n",
    "e^{\\beta_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fL_AFNK9O6LX"
   },
   "outputs": [],
   "source": [
    "coef = np.array(reg.coef_[0])\n",
    "\n",
    "#región de corte al valor de ln(1.4)\n",
    "plt.figure(figsize = (10,7))\n",
    "plt.plot(coef, np.exp(coef), '*', label = 'Importancia del coeficiente en el modelo')\n",
    "plt.axhline(y=1.4, xmin=0, xmax=1, color = 'r')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QIVzU3azS7LR"
   },
   "outputs": [],
   "source": [
    "sup_pix = coef[coef > np.log(1.4)]\n",
    "Importantes = [i for i in range(54) if coef[i] in sup_pix]\n",
    "print(Importantes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o4xZj5sKUZ7P"
   },
   "outputs": [],
   "source": [
    "# Generamos una imágen que contenga a los superpixéles más importantes:\n",
    "M_reg = np.zeros(54)\n",
    "for i in range(54):\n",
    "  if i in Importantes:\n",
    "    M_reg[i] = 1\n",
    "\n",
    "\n",
    "image_new = imagen_np.copy()\n",
    "for i in range(n_x):\n",
    "  for j in range(n_y):\n",
    "    pos = imagen_seg[i,j]\n",
    "    if M_reg[pos] == 0:\n",
    "      image_new[i,j,:] = [0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y3WITBGHV-vJ"
   },
   "outputs": [],
   "source": [
    "plt.imshow(image_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-VA9sffMYfov"
   },
   "source": [
    "A medida que aumentamos el criterio de elección de los super-pixeles (mayores coeficientes) notamos que los pixeles que se mantienen siempre entre aquellos que el modelo necesita ver para tomar una buena desición son aquellos que están cercanos a la cara del perro (en este caso). Esto es explicado por el nivel de detalle (y por lo tanto, de información) que contiene la imágen y que la diferencia de otras que puedan poseer colores u otras características similares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hSpIAvmmZtaU"
   },
   "source": [
    "La segmentación antes utilizada se hace de _manera espacial_. Es decir, se realiza una clusterización sobre la escala de grises según su posición en la imágen. Del procediminto recién explicado para implementar **LIME** reemplace la etapa de segmentación de la imágen por 2 segmentaciones espaciales utilizando 2 modelos de clustering a su elección, para ello:\n",
    "\n",
    "  11. Clusterice sobre un conjunto de entrenamiento $X$ con $299^2$ observaciones, es decir, una observación por píxel en la imagen del control escalada. Cada Observación de $X$ consta de 3 componentes, donde la primera y segunda son espaciales (posición del píxel en la imágen) y la última es el valor de intensidad asociado al píxel (escala de grises). Utilice los cluster descubiertos para generar súper-píxeles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mjrhteMrWE9n"
   },
   "outputs": [],
   "source": [
    "# Algoritmo clusterizador a ocupar: K-means\n",
    "# Luego de cargar la imágen en 2d (escala de grises), se genera una matriz de datos donde la fila corresponde al vecrtor [posición_x, posición_y, color_de_(x,y)]\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "newsize = (299,299)\n",
    "image_ = np.asarray(Image.open(filename).resize(newsize).convert('RGB'))\n",
    "image_2d = np.mean(image_, axis=2, dtype=np.uint)\n",
    "np.shape(image_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y1zbSa96Kmqr"
   },
   "outputs": [],
   "source": [
    "X_km = []\n",
    "\n",
    "for i in range(299):\n",
    "  for j in range(299):\n",
    "    X_km.append([i,j, image_2d[i,j]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aXBjJvJRcrTK"
   },
   "outputs": [],
   "source": [
    "kmean = KMeans().fit_predict(X_km)\n",
    "np.shape(kmean) # vector de etiquetas para cada vector de X_km, necesario reordenar en matrix de 299x299 para graficar vecindades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "i3082df4NHAG"
   },
   "outputs": [],
   "source": [
    "kmean_pix = kmean.reshape([299,299])\n",
    "kmean_pix.max() +1 #cantidad de clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3OM4rmLldAZv"
   },
   "outputs": [],
   "source": [
    "imagen_bnd2 = sks.mark_boundaries(image_,kmean_pix, color = (0,0,0), outline_color=(0,1,0))\n",
    "ims(imagen_bnd2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k6WIyvc8ROhQ"
   },
   "source": [
    "Podemos observar que la mayor cantidad de clusters se concentran en la cara del perro, lugar que, según el modelo anterior, era imprescindible para que la red pueda reconocer la imágen. Clusterización reconoce el pasto, el fondo y el cuerpo del perro como secciones distintas (esto es un poco evidente, puesto que los colores que predominaban en la imágen eran notoriamente distintos, por lo tanto el valor en la imágen también).\n",
    "Veamos la importancia de cada cluster visto como segmentaciones de los pasos anteriores.\n",
    "\n",
    "  12. Aplique el esquema **LIME** desarrollado anteriormente sobre sus súper-pixeles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IltxlP8fPLGE"
   },
   "outputs": [],
   "source": [
    "#probaremos con 1000 perturbaciones denuevo\n",
    "\n",
    "n_clases_km = kmean_pix.max() +1\n",
    "\n",
    "M2 = np.zeros([n_clases_km,n_per])\n",
    "\n",
    "#binomial(1,p)=bernoulli(p), asigno en una matriz de n_clases x n_perturbaciones donde cada fila corresponde al valor de la clase en esa perturbación.\n",
    "np.random.seed(10)\n",
    "for i in range(n_per):\n",
    "  M2[:,i] = np.random.binomial(1,0.5,n_clases_km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wp3N_b-iVsea"
   },
   "outputs": [],
   "source": [
    "#generando imágenes perturbadas.\n",
    "\n",
    "imag_perturb_km = [] #guardar imágenes perturbadas. guardar en formato Image\n",
    "\n",
    "\n",
    "for p in range(n_per):\n",
    "  #para cada perturbación hago una copia de la imagen\n",
    "  image_copy = image_.copy()\n",
    "  for i in range(299):\n",
    "    for j in range(299):\n",
    "      pos = kmean_pix[i,j]\n",
    "      if M2[pos,p] == 0:\n",
    "        image_copy[i,j,:] = [0,0,0]\n",
    "  \n",
    "  imag_perturb_km.append(image_copy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yCS8AW0mWLcV"
   },
   "outputs": [],
   "source": [
    "# Mostrar una imágen cualquiera \n",
    "plt.imshow(imag_perturb_km[640])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_MhH1xZ8XeXz"
   },
   "outputs": [],
   "source": [
    "#Generación del vector Y\n",
    "y_km =[]\n",
    "for i in range(n_per):\n",
    "  y_km.append(Evaluacion_red(imag_perturb_km[i],label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y5sz1tZWZEjT"
   },
   "outputs": [],
   "source": [
    "D_km = [M2[:,i] for i in range(n_per)]\n",
    "weight_km = np.array([pi_x(D_km[i],sigma) for i in range(n_per)])\n",
    "\n",
    "reg_km = LogisticRegression().fit(D_km,y_km, sample_weight=weight_km)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mIMk8F4MZhUQ"
   },
   "outputs": [],
   "source": [
    "coef_km = np.array(reg_km.coef_[0])\n",
    "\n",
    "#región de corte al valor de ln(1.4)\n",
    "plt.figure(figsize = (10,7))\n",
    "plt.plot(coef_km, np.exp(coef_km), '*', label = 'Importancia del coeficiente en el modelo')\n",
    "# plt.axhline(y=1.4, xmin=0, xmax=1, color = 'r')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jd9oeRVsby5T"
   },
   "source": [
    "Podemos ver sólo un coeficiente positivo (mayor a 6), por lo tanto al aplicar la exponencial, separa brúscamente este coeficiente de los demáses que los sitúa por debajo de 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kJxFgO6bZrUX"
   },
   "outputs": [],
   "source": [
    "sup_pix_km = coef_km[coef_km > np.log(100)]\n",
    "Importantes_km = [i for i in range(8) if coef_km[i]> np.log(100)]\n",
    "print(Importantes_km,'\\n')\n",
    "\n",
    "# Generamos una imágen que contenga a los superpixéles más importantes:\n",
    "\n",
    "coef_image = image_.copy()\n",
    "for i in range(299):\n",
    "  for j in range(299):\n",
    "    if kmean_pix[i,j] not in Importantes_km:\n",
    "      coef_image[i,j,:] = [0,0,0]\n",
    "\n",
    "plt.imshow(coef_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PLzuZ333gc6g"
   },
   "source": [
    "Al igual que la parte anterior, el aspecto más importante de la imágen es la forma que presenta la cara del perro, sin embargo (y no es muy notorio) no considera detalles de la cada, como los ojos, la nariz y el ocico. Esto puede ser por el color negro que presentan, lo que hace que anular el cluster en esa zona no presente mayor diferencia en el modelo, pues el color no cambia drásticamente como el resto del cuerpo. Bajo esa lógica es claro que el modelo de regesión no dará peso de importancia a aquellos cluster."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "P3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
