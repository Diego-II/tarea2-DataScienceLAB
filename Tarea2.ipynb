{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLyJxUxs7_AU",
        "colab_type": "text"
      },
      "source": [
        "# Tarea 2\n",
        "## Laboratorio de Datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFAopHwv7_AV",
        "colab_type": "text"
      },
      "source": [
        "# 1 Carga y transformación de Datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Na7KoK8MX8jn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Estas lineas corren solo en google colab:\n",
        "import os.path \n",
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "if IN_COLAB:\n",
        "  if os.path.exists('/content/ChestXRay2017.zip'):\n",
        "    print(\"Datos ya descargados\")\n",
        "  else:\n",
        "    !wget https://data.mendeley.com/datasets/rscbjbr9sj/2/files/f12eaf6d-6023-432f-acc9-80c9d7393433/ChestXRay2017.zip\n",
        "    !unzip /content/ChestXRay2017.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZdhQB3_7_AW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# librerías usadas\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import make_grid\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import PIL\n",
        "from skimage import io\n",
        "from skimage.color import gray2rgb\n",
        "torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 1\n",
        "# iniciar datos\n",
        "\n",
        "\"\"\"\n",
        "Funciones loaders \n",
        "- pil_loader        :   carga con pil\n",
        "- skimage_loader    :   carga con skimage\n",
        "- opencv_loader     :   carga con opencv\n",
        "\"\"\"\n",
        "def opencv_loader(path):\n",
        "    bgr_image = cv2.imread(path)\n",
        "    return bgr_image\n",
        "def pil_loader(path):\n",
        "    image_rgb =Image.open(path).convert(\"RGB\")\n",
        "    return image_rgb\n",
        "def skimage_loader(path):\n",
        "    image=io.imread(path)\n",
        "    return gray2rgb(image)\n",
        "\n",
        "div= np.iinfo('uint8').max # maximo valor de tipo uint8\n",
        "transformers=transforms.Compose(\n",
        "                [transforms.Resize([224,224]),\n",
        "                #transforms.Normalize(mean=0,std = div, inplace = True),\n",
        "                 transforms.RandomRotation(degrees=20),\n",
        "                 transforms.RandomHorizontalFlip(),\n",
        "                 transforms.ColorJitter(brightness=[1.2, 1.5]),\n",
        "                 transforms.ToTensor() # ToTensor convierte a valores entre 0 y 1\n",
        "                ])\n",
        "\n",
        "# Usamos el loader por defecto de ImageFolder\n",
        "# Deja las imagenes con 3 capas\n",
        "from torchvision.datasets.folder import default_loader\n",
        "    \n",
        "import google.colab\n",
        "import os.path \n",
        "root = '/content/chest_xray/train/'\n",
        "data_train1=datasets.DatasetFolder(root=root,loader=default_loader,transform=transformers, extensions='jpeg')\n",
        "data_test1=datasets.DatasetFolder(root=root,loader=default_loader,transform=transformers, extensions='jpeg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_-rzRWz7_Ab",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "To Do : \n",
        "\n",
        "\n",
        "Perfilamiento de tiempo de cómputo \n",
        "\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMasDQmB7_Aj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "c568e4ce-0a6c-4efd-ddb2-4b300b5b4485"
      },
      "source": [
        "# 2\n",
        "\n",
        "classes_train1 = data_train1.targets\n",
        "classes_test1 = data_test1.targets\n",
        "\n",
        "pneumonia_train=int(sum(classes_train1))\n",
        "normal_train=len(classes_train1)-pneumonia_train\n",
        "\n",
        "\n",
        "pneumonia_test=int(sum(classes_test1))\n",
        "normal_test=len(classes_test1)-pneumonia_test\n",
        "\n",
        "labels = 'Normal', 'Pneumonia'\n",
        "sizes_train = [normal_train, pneumonia_train]\n",
        "\n",
        "sizes_test = [normal_test, pneumonia_test]\n",
        "\n",
        "fig, axs = plt.subplots(1,2)\n",
        "axs[0].pie(sizes_train,labels=labels,explode=(0,0.1),autopct='%1.1f%%')\n",
        "axs[1].pie(sizes_test,labels=labels,explode =(0,0.1),autopct='%1.1f%%')\n",
        "\n",
        "axs[0].set_title('Train set')\n",
        "axs[1].set_title('Test set')\n",
        "\n",
        "\n",
        "plt.show()\n",
        "plt.savefig('distribucionTrainTest.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzpzG5SB7_An",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 3\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "# split\n",
        "train_idx, val_idx = train_test_split(list(range(len(data_train1))),test_size=0.2)\n",
        "#data_train = Subset(data_train1, train_idx)\n",
        "#data_val   = Subset(data_train1,val_idx)\n",
        "\n",
        "class ReplicarMuestreoDePrueba(torch.utils.data.Sampler):\n",
        "    \n",
        "    def __init__(self,etiquetas_prueba, indices_val, etiquetas_val):\n",
        "        self.indices_val      = indices_val\n",
        "        #self.etiquetas_val    = etiquetas_val\n",
        "        self.prob_pneumonia   = sum(etiquetas_prueba)/len(etiquetas_prueba)\n",
        "        self.prob_normal      = 1-self.prob_pneumonia\n",
        "        self.prob_vector      = [ int((etiquetas_val[i]==1 )*self.prob_pneumonia+\n",
        "                                 (etiquetas_val[i]==0)*self.prob_normal)\n",
        "                                for i in range(len(etiquetas_val))\n",
        "                                ]\n",
        "    def __iter__(self):\n",
        "        return iter(np.random.choice(self.indices_val,p=self.prob_vector))\n",
        "    \n",
        "etiquetas_prueba = data_test1.targets\n",
        "# indices_val = val_idx\n",
        "etiquetas_val = [data_train1.targets[i] for i in val_idx ] \n",
        "\n",
        "a=ReplicarMuestreoDePrueba(etiquetas_prueba,val_idx, etiquetas_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beJIPnQb7_At",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 4\n",
        "from torch.utils.data.sampler import RandomSampler, SubsetRandomSampler\n",
        "\n",
        "data_train = DataLoader(data_train1,sampler=SubsetRandomSampler(train_idx))\n",
        "data_val   = DataLoader(data_train1,sampler=ReplicarMuestreoDePrueba(etiquetas_prueba,val_idx, etiquetas_val))\n",
        "data_test  = DataLoader(data_test1,sampler=RandomSampler(data_test1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQeIVJFT7_Ax",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "232d1adc-6191-4035-c945-852763968fab"
      },
      "source": [
        "tensor_image = data_train1[0][0].permute(1, 2, 0)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(tensor_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkZtDsw-7_A1",
        "colab_type": "text"
      },
      "source": [
        "# P2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxHrOK5h7_A2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#1 \n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(2020)\n",
        "\n",
        "\n",
        "class DWSepConv2d(nn.Module):\n",
        "    \n",
        "    def __init__(self,in_channels, out_channels, kernel_size,padding,bias=True):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels,kernel_size,padding=padding,bias=bias)\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels,kernel_size=1,padding=padding,bias=bias)\n",
        "    def forward(self,xb):\n",
        "        xb = F.relu(self.conv1(xb.float()))\n",
        "        xb = F.relu(self.conv2(xb)) \n",
        "        return xb"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtSek5x67_A6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#2\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "class VGG16DWSep(nn.Module):\n",
        "    \n",
        "    def __init__(self,in_channels):\n",
        "        super().__init__()\n",
        "        # bloque 1\n",
        "        self.conv1 = nn.Conv2d(in_channels,64,kernel_size=3,padding=1,stride=1)\n",
        "        self.conv2 = nn.Conv2d(64,64,kernel_size=3, padding=1, stride=1)\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2,stride=2) \n",
        "        # bloque 2\n",
        "        self.dwconv3 = DWSepConv2d(64,128,kernel_size=3,padding=1)\n",
        "        self.dwconv4 = DWSepConv2d(128,128,kernel_size=3,padding=1)\n",
        "        self.maxpool2 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "        # bloque 3        \n",
        "        self.dwconv5 = DWSepConv2d(128,256,kernel_size=3,padding=1)\n",
        "        self.batchnorm1 = nn.BatchNorm2d(256)\n",
        "        # bloque 4\n",
        "        self.dwconv6 = DWSepConv2d(256,256,kernel_size=3,padding=1)\n",
        "        self.batchnorm2 = nn.BatchNorm2d(256)\n",
        "        # bloque 5\n",
        "        self.dwconv7 = DWSepConv2d(256,256,kernel_size=3,padding=1)\n",
        "        self.maxpool3 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "        # bloque 6\n",
        "        self.dwconv8 = DWSepConv2d(256,512,kernel_size=3,padding=1)\n",
        "        self.batchnorm3 = nn.BatchNorm2d(512)\n",
        "        # bloque 7\n",
        "        self.dwconv9 = DWSepConv2d(512,512,kernel_size=3,padding=1)\n",
        "        self.batchnorm4 = nn.BatchNorm2d(512)\n",
        "        # bloque 8\n",
        "        self.dwconv10 = DWSepConv2d(512,512,kernel_size=3,padding=1)\n",
        "        self.maxpool4 = nn.MaxPool2d(kernel_size=2,stride=2)\n",
        "        # bloque o1\n",
        "        self.flatten1 = nn.Flatten()\n",
        "        self.lin1 = nn.Linear(739328,1024)\n",
        "        self.drop1 = nn.Dropout(.7)\n",
        "        # bloque o2\n",
        "        self.lin2 = nn.Linear(1024,512)\n",
        "        self.drop2 = nn.Dropout(.5)\n",
        "        self.lin3 = nn.Linear(512,2)\n",
        "    \n",
        "    def forward(self,xb):\n",
        "      # bloque 1\n",
        "      xb = xb.view(-1, 3, 224, 224)\n",
        "      xb = F.relu(self.conv1(xb))\n",
        "      xb = F.relu(self.conv2(xb))\n",
        "      xb = self.maxpool1(xb)\n",
        "      # bloque 2\n",
        "      xb = F.relu(self.dwconv3(xb))\n",
        "      xb = F.relu(self.dwconv4(xb))\n",
        "      xb = self.maxpool2(xb)\n",
        "      # bloque 3\n",
        "      xb = F.relu(self.dwconv5(xb))\n",
        "      xb = F.relu(self.batchnorm1(xb))\n",
        "      # bloque 4\n",
        "      xb = F.relu(self.dwconv6(xb))\n",
        "      xb = F.relu(self.batchnorm2(xb))\n",
        "      # bloque 5\n",
        "      xb = F.relu(self.dwconv7(xb))\n",
        "      xb = self.maxpool3(xb)\n",
        "      # bloque 6\n",
        "      xb = F.relu(self.dwconv8(xb))\n",
        "      xb = F.relu(self.batchnorm3(xb))\n",
        "      # bloque 7\n",
        "      xb = F.relu(self.dwconv9(xb))\n",
        "      xb = F.relu(self.batchnorm4(xb))\n",
        "      # bloque 8\n",
        "      xb = F.relu(self.dwconv10(xb))\n",
        "      # bloque o1\n",
        "      xb = self.flatten1(xb)\n",
        "      xb = F.relu(self.lin1(xb))\n",
        "      xb = self.drop1(xb)\n",
        "      # bloque o2\n",
        "      xb = F.relu(self.lin2(xb))\n",
        "      xb = self.drop2(xb)\n",
        "      xb = F.relu(self.lin3(xb))\n",
        "      return xb.view(-1, xb.size(1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "gN9I9dKK7_A-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Descargamos la red vgg16\n",
        "vgg16 = torchvision.models.vgg16(pretrained=True, progress=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13DYFt_N7_BF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 963
        },
        "outputId": "5c0a7455-1c45-428b-d5d3-a2f8cd823582"
      },
      "source": [
        "from torchsummary import summary\n",
        "summary(vgg16.cuda(),(3,224,244))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_C6h5Ziz7Fpm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Se quiere la capa 0 y la 2, que son las dos primeras convolucionales:\n",
        "pesos_dict = {\n",
        "    'conv1' : vgg16.features[0],\n",
        "    'conv2' : vgg16.features[2]\n",
        "}\n",
        "\n",
        "\n",
        "net = VGG16DWSep(in_channels = 3)\n",
        "\n",
        "#intento de traspaso de pesos:\n",
        "net.conv1.weight = pesos_dict['conv1'].weight\n",
        "net.conv2.weight = pesos_dict['conv2'].weight\n",
        "\n",
        "\n",
        "# Congelamos los pesos de la red vgg16\n",
        "for param in vgg16.parameters():\n",
        "  param.requires_grad = False\n",
        "\n",
        "# Congelamos los pesos de las dos primeras capas convoluvionales de la red \n",
        "# (capas que se transfirieron):\n",
        "net.conv1.requires_grad_ = False\n",
        "net.conv2.requires_grad_ = False\n",
        "\n",
        "\n",
        "# Se elimina la red VGG16 para liberar espacio en colab \n",
        "del vgg16\n",
        "torch.cuda.empty_cache()\n",
        "# Show GPU Stat\n",
        "!gpustat -p\n",
        "\n",
        "# Se requiere la libreria numpy!!\n",
        "import numpy  as np\n",
        "class EarlyStopping():\n",
        "  '''\n",
        "  Regularization heuristic:\n",
        "\n",
        "  '''\n",
        "  def __init__(self, modo='min', paciencia=5, porcentaje:bool = False, tol=0):\n",
        "    '''\n",
        "    Arguments:\n",
        "    ---------\n",
        "    modo: 'min' o 'max'. Si se debe minimizar o maximizar la metrica objetivo\n",
        "    paciancia: Cantidad de epocas en la que la metrica puede empeorar\n",
        "    porcentaje: si la diferencia es relativa (true) o absoluta\n",
        "    tol: diferencia minima que debe existir con respecto la mejor metrica ya\n",
        "        observada  para considerar si existe un empeoramiento del desempeno\n",
        "    '''\n",
        "    self.modo = modo\n",
        "    self.paciencia  = paciencia\n",
        "    self.porcentaje = porcentaje\n",
        "    self.best = np.Inf if self.modo == 'min' else -np.Inf\n",
        "    self.contador = 0\n",
        "    self.tol = tol\n",
        "\n",
        "  \n",
        "  def __compareMin(self, metrica_validacion):\n",
        "    if self.porcentaje:      \n",
        "      # Si la dif relativa es mayor a la tolerada: actualizar contador:\n",
        "      if metrica_validacion < (1-self.tol)*self.best:\n",
        "        self.contador = 0\n",
        "        self.best = metrica_validacion\n",
        "        return True\n",
        "      else:\n",
        "        self.contador +=1\n",
        "        return False\n",
        "    else:\n",
        "      if metrica_validacion < self.best - self.tol:\n",
        "        self.best = metrica_validacion\n",
        "        self.contador = 0\n",
        "        return True\n",
        "      else:\n",
        "        self.contador += 1\n",
        "        return False\n",
        "    \n",
        "    def __comareMax(self, metrica_validacion):\n",
        "      if self.porcentaje:\n",
        "         # Si la dif relativa es mayor a la tolerada: actualizar contador:\n",
        "        if metrica_validacion > (1+self.tol)*self.best:\n",
        "          self.contador = 0\n",
        "          self.best = metrica_validacion\n",
        "          return True\n",
        "        else:\n",
        "          self.contador +=1\n",
        "          return False\n",
        "      else:\n",
        "        if metrica_validacion > self.best + self.tol:\n",
        "          self.best = metrica_validacion\n",
        "          self.contador = 0\n",
        "          return True\n",
        "        else:\n",
        "          self.contador += 1\n",
        "          return False        \n",
        "\n",
        "  # Es necesaria la anotacion??\n",
        "  #@classmethod\n",
        "  def __mejor(self, metrica_validacion):\n",
        "    '''\n",
        "    Compara @metrica_validacion con la mejor ya observada segun las \n",
        "    especificaciones de porcentaje y modo. \n",
        "    '''\n",
        "    if self.modo == 'min':\n",
        "      # Comparar segun el modo y porcentaje:\n",
        "      if self.__compareMin(metrica_validacion):\n",
        "        return True\n",
        "      else:\n",
        "        return False\n",
        "    else:\n",
        "      # Comparar segun el modo y porcentaje:\n",
        "      if self.__compareMax(metrica_validacion):\n",
        "        return True\n",
        "      else:\n",
        "        return False\n",
        "\n",
        "  #@classmethod\n",
        "  def deberia_parar(self, metrica_validacion):\n",
        "    if not self.__mejor(metrica_validacion) and self.contador >= self.paciencia:\n",
        "      return True\n",
        "    else:\n",
        "      return False\n",
        "\n",
        "def imshow(inp, title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "\n",
        "# Get a batch of training data\n",
        "#inputs, classes = next(iter(dataloaders['train']))\n",
        "\n",
        "# Make a grid from batch\n",
        "#out = torchvision.utils.make_grid(inputs)\n",
        "\n",
        "#imshow(out, title=[class_names[x] for x in classes])\n",
        "\n",
        "\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1lssOMVb7dAp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#num_ftrs = net.fc.in_features\n",
        "# Here the size of each output sample is set to 2.\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "#model_ft.fc = nn.Linear(num_ftrs, 2)\n",
        "\n",
        "model_ft = net.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "#optimizer_ft = optim.Adam(model_ft.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=1, gamma=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJZLMzx57drn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTbRtp417_Bo",
        "colab_type": "text"
      },
      "source": [
        "# P3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kpV7ENt7_Bp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_AcgKVQ7_Bs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#1\n",
        "\n",
        "# librerías usadas\n",
        "import torch\n",
        "from torchvision import models\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import make_grid\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# iniciar datos\n",
        "def load(path):\n",
        "    image_rgb =Image.open(path).convert(\"RGB\")\n",
        "    return image_rgb\n",
        "\n",
        "\n",
        "\n",
        "# transformaciones compose\n",
        "transformers=transforms.Compose([transforms.Resize([229,229]),\n",
        "                transforms.CenterCrop(229),\n",
        "                 transforms.ToTensor(),\n",
        "                 transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225], inplace = True)\n",
        "                ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "JFYoa1by7_Bv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 2\n",
        "from keras.applications.imagenet_utils import decode_predictions\n",
        "inception_v3_net = models.inception_v3(pretrained=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "IMdFBr5o7_By",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "inception_v3_net.eval()\n",
        "filename = 'dog.jpg'\n",
        "input_image = Image.open(filename)\n",
        "input_tensor = transformers(input_image).float()\n",
        "input_batch = input_tensor.unsqueeze(0)\n",
        "inception_v3_net(input_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdxVjr7Oo8Ex",
        "colab_type": "text"
      },
      "source": [
        "# P3 Interpretabilidad"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6YzozNGo_XH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#1 \n",
        "# librerías usadas\n",
        "import torch\n",
        "from torchvision import models\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import make_grid\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# iniciar datos\n",
        "def load(path):\n",
        "    image_rgb =Image.open(path).convert(\"RGB\")\n",
        "    return image_rgb\n",
        "\n",
        "\n",
        "\n",
        "# transformaciones compose\n",
        "transformers=transforms.Compose(\n",
        "    [transforms.Resize(299),\n",
        "     transforms.CenterCrop(229),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])         \n",
        "    ]\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OvcoiOhmpE5h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#2\n",
        "#imagen sde prueba\n",
        "%%capture\n",
        "import urllib\n",
        "url, filename = (\"https://github.com/pytorch/hub/raw/master/dog.jpg\", \"dog.jpg\")\n",
        "try: urllib.URLopener().retrieve(url, filename)\n",
        "except: urllib.request.urlretrieve(url, filename)\n",
        "\n",
        "### cargo el modelo inception_v3\n",
        "inception_v3_net = torch.hub.load('pytorch/vision:v0.6.0', 'inception_v3', pretrained=True)\n",
        "\n",
        "###genero el modelo \n",
        "inception_v3_net.eval()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x9H4-qUCpJcC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transformo los datos\n",
        "input_image = Image.open(filename)\n",
        "plt.imshow(np.asarray(input_image))\n",
        "\n",
        "input_tensor = transformers(input_image)\n",
        "input_batch = input_tensor.unsqueeze(0)\n",
        "\n",
        "# # move the input and model to GPU for speed if available\n",
        "# if torch.cuda.is_available():\n",
        "#     input_batch = input_batch.to('cuda')\n",
        "#     model.to('cuda')\n",
        "\n",
        "# with torch.no_grad():\n",
        "#   output = model(input_batch)\n",
        "\n",
        "#predicción del modelo\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    input_batch = input_batch.to('cuda')\n",
        "    inception_v3_net.to('cuda')\n",
        "    \n",
        "with torch.no_grad():\n",
        "  out = inception_v3_net(input_batch).cpu()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2PdftDeZpLPH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.applications.imagenet_utils import decode_predictions\n",
        "labs = decode_predictions(out.numpy(),top=1)\n",
        "\n",
        "print('Nombre de la clase predicha: ', labs[0][0][0],'\\n')\n",
        "print('Descripción de la clase: ', labs[0][0][1],'\\n')\n",
        "print('Valor del puntaje de la red: ',labs[0][0][2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cndNkCdEpOcP",
        "colab_type": "text"
      },
      "source": [
        "  3. Segmente la imagen de control utilizando la función ```slic``` del módulo ```skimage.segmentation```, para los parámetros ```start_label=0```, ```n_segments=80```. El resultado de la segmentación es un arreglo de dimensión 299&times;299 que asigna una categoría para cada píxel de la imágen procesada. Todos los pixeles en la imágen que comparten etiqueta conforman un súper-píxel dentro de la imágen. Utilice la función ```mark_boundaries``` del módulo ```skimage.segmentation``` en conjunto con ```imshow``` del módulo ```skimage.io``` para visualizar los bordes inducidos por el conjunto de super-pixeles. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCMrsa4dpQV4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import skimage.segmentation as sks\n",
        "from skimage.io import imshow as ims\n",
        "\n",
        "imagen_np = np.asarray(input_image)\n",
        "imagen_seg = sks.slic(imagen_np, n_segments=80)\n",
        "\n",
        "print('Imágen segmentada:\\n')\n",
        "ims(imagen_seg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nn_KfNDbqmFd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Imagen original:\\n')\n",
        "plt.imshow(imagen_np)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bXrDYGtqnoc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imagen_bnd = sks.mark_boundaries(imagen_np,imagen_seg, color = (0,0,0), outline_color=(0,1,0))\n",
        "print('Imágen con los super-pixeles marcados en sus bordes:\\n')\n",
        "ims(imagen_bnd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWbcMFR8xxnf",
        "colab_type": "text"
      },
      "source": [
        "Al representar una imágen _x_ por medio de la presencia y ausencia de súper-pixeles se logra una representación interpretable _x'_ según un vector de entradas binarias.\n",
        "\n",
        "Genere perturbaciones en la imágen de control, para esto siga los siguientes pasos:\n",
        "\n",
        "  4. Defina un número de _perturbaciones_ a realizar (al menos 1000). Cada perturbación consiste en un arreglo binario, donde cada componente es asociada a un súper-pixel. Estos arreglos serán las representaciones interpretables de la imagen de control (_x'_ asociado a _x_). Considere cada entrada del arreglo de perturbaciones como una variable **Bernoulli** con _p=0.5_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouOx_Tzjxyik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#probaremos con 1000 perturbaciones\n",
        "n_per = 1000\n",
        "ind = np.array([(i in imagen_seg) for i in range(80)])\n",
        "n_clases = len(ind[ind == True])\n",
        "\n",
        "M = np.zeros([n_clases,n_per])\n",
        "\n",
        "#binomial(1,p)=bernoulli(p), asigno en una matriz de n_clases x n_perturbaciones donde cada fila corresponde al valor de la clase en esa perturbación.\n",
        "np.random.seed(10)\n",
        "for i in range(n_per):\n",
        "  M[:,i] = np.random.binomial(1,0.5,n_clases)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQYaWAdpx2b5",
        "colab_type": "text"
      },
      "source": [
        "  5. Genere tantas versiones perturbadas de la imagen de control como perturbaciones haya conseguido. Obtener una imagen perturbada consiste en asignar el valor 0 en cada canal de color en aquellos píxeles cuyos super-pixeles asociados tengan su componente nula en el vector de perturbaciones. Obtenga una visualización de una imágen perturbada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8oizspwx3YL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "outputId": "c7e37a14-6381-41b6-c9ad-657823bc7ecc"
      },
      "source": [
        "#generando imágenes perturbadas.\n",
        "\n",
        "n_x, n_y, n_colores= np.shape(imagen_np) #coordenadas de la imagen normal.\n",
        "\n",
        "imag_perturb = [] #guardar imágenes perturbadas. guardar en formato Image\n",
        "\n",
        "\n",
        "for p in range(n_per):\n",
        "  #para cada perturbación hago una copia de la imagen\n",
        "  image_copy = imagen_np.copy()\n",
        "  for i in range(n_x):\n",
        "    for j in range(n_y):\n",
        "      pos = imagen_seg[i,j]\n",
        "      if M[pos,p] == 0:\n",
        "        image_copy[i,j,:] = [0,0,0]\n",
        "  \n",
        "  imag_perturb.append(image_copy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCHRWUqtzoxp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mostrar una imágen cualquiera\n",
        "plt.imshow(imag_perturb[425])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEu8DYMEzrK-",
        "colab_type": "text"
      },
      "source": [
        "imágenes perturbadas  que sean clasificadas a la misma categoría de la imágen de control y 0 en caso contrario, el arreglo binario correspondiente se denotará _y_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjXzhHMVzsbA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#función que evalúa una imágen y la compara con una etiqueta según la predicción\n",
        "def Evaluación_red(imag_array, nombre_cat, print_ = False):\n",
        "  input_image = Image.fromarray(imag_array)\n",
        "  input_tensor = transformers(input_image)\n",
        "  input_batch = input_tensor.unsqueeze(0)\n",
        "\n",
        "  if torch.cuda.is_available():\n",
        "    input_batch = input_batch.to('cuda')\n",
        "    inception_v3_net.to('cuda')\n",
        "    \n",
        "  with torch.no_grad():\n",
        "    out = inception_v3_net(input_batch).cpu()\n",
        "  \n",
        "  labs = decode_predictions(out.numpy(),top=1)\n",
        "  \n",
        "  if print_:\n",
        "    print(labs[0][0][0])\n",
        "  \n",
        "  if labs[0][0][0] == nombre_cat:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxmh8wLKzwIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# nombre de la categoría correcta.\n",
        "label = labs[0][0][0]\n",
        "label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iy771ghDzxb8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Prueba de la función\n",
        "Evaluación_red(imag_perturb[20], label, print_ = True) #no acertó"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SId27RR7zzNA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Generación del vector Y\n",
        "y =[]\n",
        "for i in range(n_per):\n",
        "  y.append(Evaluación_red(imag_perturb[i],label))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJSNoPFaz8b4",
        "colab_type": "text"
      },
      "source": [
        "  7. Calcule &pi;<sub>_x_</sub> según la expresión (3). Para ello, obtenga la distancia del coseno entre las perturbaciones asociadas a cada imágen perturbada y el vector de perturbación de la imágen de control _x_ según lo indica (4).\n",
        "\n",
        "\n",
        "\n",
        "  Tomamos\n",
        "$$\n",
        "x' = (1,\\cdots,1)\n",
        "$$\n",
        "como el vector que identifica a la imagen original $x$. Entonces, si $N$ es la cantidad de clases encontradas en la segmentación, entonces:\n",
        "\n",
        "$$\n",
        "x' \\in \\{0,1\\}^N,\\hspace{0.4cm}\\text{y }\\hspace{0.4cm}||x'|| = \\sqrt{N}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHgJooZmz9Nz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Definimos la distancia.\n",
        "def dist_x(z):\n",
        "  n = len(z)\n",
        "  a = np.sum(z)\n",
        "  norm_x = np.sqrt(n)\n",
        "  norm_z = np.sqrt(np.sum(z**2))\n",
        "\n",
        "  d = 1 - (a/(norm_x*norm_z))\n",
        "  if norm_z == 0:\n",
        "    return 1\n",
        "  else:\n",
        "    return d\n",
        "\n",
        "# Definimos la distancia coseno:\n",
        "\n",
        "def pi_x(z,sigma):\n",
        "  d = dist_x(z)\n",
        "\n",
        "  return np.exp((-d**2) / (sigma**2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4hWVtwG0Dgs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Primera perturbación:\n",
        "per_0 = M[:,0]\n",
        "np.shape(per_0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1g4owxeA0FMI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prueba de las funciones anteriores para sigma=1.\n",
        "pi_x(per_0, sigma=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJtyfhB_0Ifw",
        "colab_type": "text"
      },
      "source": [
        "Una vez obtenido el conjunto de reprsentaciones para la imágen de control $x$ y el vector de pesos asociados &pi;<sub>$x$</sub>, se pasa a minimizar la función de fidelidad, para esto:\n",
        "\n",
        "  8. Genere un conjunto de entrenamiento $D_p$. Este consta de vectores de perturbación como observaciones. La variable de respuesta será el arreglo $y$ generado anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LWXgF_h0JBI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "D_p = [M[:,i] for i in range(n_per)]\n",
        "\n",
        "# y definido anteriormente."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_G5fDQJv0P0p",
        "colab_type": "text"
      },
      "source": [
        "  9. Utilice la clase ```LogisticRegression``` del módulo ```sklearn.linear_model``` para entrenar un clasificador sobre el conjunto de entrenamiento $D_p$. Haga uso de $\\pi_x$ al momento de usar el método ```.fit()```. ¿Es posible agregar una medida de complejidad $\\Omega(g)$ con este esquema?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5m8-MGzQ36a0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "sigma = 0.25\n",
        "weight = np.array([pi_x(D_p[i],sigma) for i in range(n_per)])\n",
        "\n",
        "reg = LogisticRegression().fit(D_p,y, sample_weight=weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juDFHPTy38RR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reg.coef_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtrWt2VO3_Az",
        "colab_type": "text"
      },
      "source": [
        "   10. Utilice los coeficientes del clasificador anterior para inferir los súper-índices de mayor importancia en la clasificación de la imágen de control."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPRXperC4CNH",
        "colab_type": "text"
      },
      "source": [
        "Dado que la regresión logística vincula a la variable $X$ con la respuesta $Y$ de una forma no lineal, no se analizarán a los coeficientes por el tamaño de su magnitud (módulo) como en la regresión lineal. En cambio, la importancia de la variable estará dada por un factor exponencial del coeficiente. Es decir, si $\\beta_i$ es el coeficiente asociado al súper-píxel $i$, entonces la importancia de tal súperpixel se puede medir mediante:\n",
        "\n",
        "$$\n",
        "e^{\\beta_i}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6HbNT6_4D-W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "coef = np.array(reg.coef_[0])\n",
        "\n",
        "#región de corte al valor de ln(1.4)\n",
        "plt.figure(figsize = (10,7))\n",
        "plt.plot(coef, np.exp(coef), '*', label = 'Importancia del coeficiente en el modelo')\n",
        "plt.axhline(y=1.4, xmin=0, xmax=1, color = 'r')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYpIbLbQ4GBL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sup_pix = coef[coef > np.log(1.4)]\n",
        "Importantes = [i for i in range(54) if coef[i] in sup_pix]\n",
        "print(Importantes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlR_fw3k4IIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generamos una imágen que contenga a los superpixéles más importantes:\n",
        "M_reg = np.zeros(54)\n",
        "for i in range(54):\n",
        "  if i in Importantes:\n",
        "    M_reg[i] = 1\n",
        "\n",
        "\n",
        "image_new = imagen_np.copy()\n",
        "for i in range(n_x):\n",
        "  for j in range(n_y):\n",
        "    pos = imagen_seg[i,j]\n",
        "    if M_reg[pos] == 0:\n",
        "      image_new[i,j,:] = [0,0,0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiRnHAM74Jon",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.imshow(image_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpKLJ8Ej4LqQ",
        "colab_type": "text"
      },
      "source": [
        "A medida que aumentamos el criterio de elección de los super-pixeles (mayores coeficientes) notamos que los pixeles que se mantienen siempre entre aquellos que el modelo necesita ver para tomar una buena desición son aquellos que están cercanos a la cara del perro (en este caso). Esto es explicado por el nivel de detalle (y por lo tanto, de información) que contiene la imágen y que la diferencia de otras que puedan poseer colores u otras características similares."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrJY2Wj-4NK6",
        "colab_type": "text"
      },
      "source": [
        "La segmentación antes utilizada se hace de _manera espacial_. Es decir, se realiza una clusterización sobre la escala de grises según su posición en la imágen. Del procediminto recién explicado para implementar **LIME** reemplace la etapa de segmentación de la imágen por 2 segmentaciones espaciales utilizando 2 modelos de clustering a su elección, para ello:\n",
        "\n",
        "  11. Clusterice sobre un conjunto de entrenamiento $X$ con $299^2$ observaciones, es decir, una observación por píxel en la imagen del control escalada. Cada Observación de $X$ consta de 3 componentes, donde la primera y segunda son espaciales (posición del píxel en la imágen) y la última es el valor de intensidad asociado al píxel (escala de grises). Utilice los cluster descubiertos para generar súper-píxeles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXqn668X4MWu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Algoritmo clusterizador a ocupar: K-means\n",
        "# Luego de cargar la imágen en 2d (escala de grises), se genera una matriz de datos donde la fila corresponde al vecrtor [posición_x, posición_y, color_de_(x,y)]\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "newsize = (299,299)\n",
        "image_ = np.asarray(Image.open(filename).resize(newsize).convert('RGB'))\n",
        "image_2d = np.mean(image_, axis=2, dtype=np.uint)\n",
        "np.shape(image_2d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujPTrxvj4R_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_km = []\n",
        "\n",
        "for i in range(299):\n",
        "  for j in range(299):\n",
        "    X_km.append([i,j, image_2d[i,j]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hB-vuUjC4Txy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kmean = KMeans().fit_predict(X_km)\n",
        "np.shape(kmean) # vector de etiquetas para cada vector de X_km, necesario reordenar en matrix de 299x299 para graficar vecindades"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Js-INkE4YQg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "kmean_pix = kmean.reshape([299,299])\n",
        "kmean_pix.max() +1 #cantidad de clases"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5j_3kcTJ4tZf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imagen_bnd2 = sks.mark_boundaries(image_,kmean_pix, color = (0,0,0), outline_color=(0,1,0))\n",
        "ims(imagen_bnd2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoet73Vr4uia",
        "colab_type": "text"
      },
      "source": [
        "Podemos observar que la mayor cantidad de clusters se concentran en la cara del perro, lugar que, según el modelo anterior, era imprescindible para que la red pueda reconocer la imágen. Clusterización reconoce el pasto, el fondo y el cuerpo del perro como secciones distintas (esto es un poco evidente, puesto que los colores que predominaban en la imágen eran notoriamente distintos, por lo tanto el valor en la imágen también).\n",
        "Veamos la importancia de cada cluster visto como segmentaciones de los pasos anteriores.\n",
        "\n",
        "  12. Aplique el esquema **LIME** desarrollado anteriormente sobre sus súper-pixeles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fh8deSJn4xnc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#probaremos con 1000 perturbaciones denuevo\n",
        "\n",
        "n_clases_km = kmean_pix.max() +1\n",
        "\n",
        "M2 = np.zeros([n_clases_km,n_per])\n",
        "\n",
        "#binomial(1,p)=bernoulli(p), asigno en una matriz de n_clases x n_perturbaciones donde cada fila corresponde al valor de la clase en esa perturbación.\n",
        "np.random.seed(10)\n",
        "for i in range(n_per):\n",
        "  M2[:,i] = np.random.binomial(1,0.5,n_clases_km)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlJusWaw4y1B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#generando imágenes perturbadas.\n",
        "\n",
        "imag_perturb_km = [] #guardar imágenes perturbadas. guardar en formato Image\n",
        "\n",
        "\n",
        "for p in range(n_per):\n",
        "  #para cada perturbación hago una copia de la imagen\n",
        "  image_copy = image_.copy()\n",
        "  for i in range(299):\n",
        "    for j in range(299):\n",
        "      pos = kmean_pix[i,j]\n",
        "      if M2[pos,p] == 0:\n",
        "        image_copy[i,j,:] = [0,0,0]\n",
        "  \n",
        "  imag_perturb_km.append(image_copy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBNVASeA40fx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mostrar una imágen cualquiera \n",
        "plt.imshow(imag_perturb_km[640])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WllPsTvy41dC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Generación del vector Y\n",
        "y_km =[]\n",
        "for i in range(n_per):\n",
        "  y_km.append(Evaluación_red(imag_perturb_km[i],label))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIuGi2Y642W4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "D_km = [M2[:,i] for i in range(n_per)]\n",
        "weight_km = np.array([pi_x(D_km[i],sigma) for i in range(n_per)])\n",
        "\n",
        "reg_km = LogisticRegression().fit(D_km,y_km, sample_weight=weight_km)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TE9wqDYy44Fr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "coef_km = np.array(reg_km.coef_[0])\n",
        "\n",
        "#región de corte al valor de ln(1.4)\n",
        "plt.figure(figsize = (10,7))\n",
        "plt.plot(coef_km, np.exp(coef_km), '*', label = 'Importancia del coeficiente en el modelo')\n",
        "# plt.axhline(y=1.4, xmin=0, xmax=1, color = 'r')\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVGHZ2Kk46Wh",
        "colab_type": "text"
      },
      "source": [
        "Podemos ver sólo un coeficiente positivo (mayor a 6), por lo tanto al aplicar la exponencial, separa brúscamente este coeficiente de los demáses que los sitúa por debajo de 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdCJHcRx473N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sup_pix_km = coef_km[coef_km > np.log(100)]\n",
        "Importantes_km = [i for i in range(8) if coef_km[i]> np.log(100)]\n",
        "print(Importantes_km,'\\n')\n",
        "\n",
        "# Generamos una imágen que contenga a los superpixéles más importantes:\n",
        "\n",
        "coef_image = image_.copy()\n",
        "for i in range(299):\n",
        "  for j in range(299):\n",
        "    if kmean_pix[i,j] not in Importantes_km:\n",
        "      coef_image[i,j,:] = [0,0,0]\n",
        "\n",
        "plt.imshow(coef_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWIKISdK4-0w",
        "colab_type": "text"
      },
      "source": [
        "Al igual que la parte anterior, el aspecto más importante de la imágen es la forma que presenta la cara del perro, sin embargo (y no es muy notorio) no considera detalles de la cada, como los ojos, la nariz y el ocico. Esto puede ser por el color negro que presentan, lo que hace que anular el cluster en esa zona no presente mayor diferencia en el modelo, pues el color no cambia drásticamente como el resto del cuerpo. Bajo esa lógica es claro que el modelo de regesión no dará peso de importancia a aquellos cluster.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjqLMKmX4_tf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3-final"
    },
    "colab": {
      "name": "Tarea2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}